# Semantic Segmentation with Fully Convolutional Networks (FCN)

1. TOC
{:toc}

The project model which will be used for transfer learning and domain adaptation is DeepLabv3FineTuning. 
* <https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/>
* <https://github.com/msminhas93/DeepLabv3FineTuning>

Then we will be using the tutorial to infer an image with the new trained model.
* <https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/>

Then to save it as an ONNX file and use it, check this tutorial:
* <https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html>
* <https://pytorch.org/docs/stable/onnx.html>

Maybe taking a look at (NVidia) DeepStream
* <https://towardsdatascience.com/how-to-deploy-onnx-models-on-nvidia-jetson-nano-using-deepstream-b2872b99a031>


Références:
* <https://github.com/dusty-nv/pytorch-segmentation>
* <https://github.com/pytorch/vision/tree/v0.3.0/references/segmentation>
* <https://github.com/DeepSceneSeg/AdapNet>
* <https://github.com/tensorflow/models/tree/master/research/slim>
* <https://www.jeremyjordan.me/semantic-segmentation/>
* <https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef>
* <http://www.cvlibs.net/datasets/kitti/eval_road.php>
* <https://www.researchgate.net/post/What_exactly_is_the_label_data_set_for_semantic_segmentation_using_FCN>
* (pdf) <https://www.researchgate.net/publication/334290613_Semantic_Segmentation_with_Deep_Learning_A_general_introduction>
* <https://pytorch.org/blog/torchvision03/>
* cheatsheet vocabulary tips and tricks <https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks>


## Préparation 
* Téléchargement du projet à partir de github en zip
```
wget https://github.com/msminhas93/DeepLabv3FineTuning/archive/master.zip
mv /home/lefv2603/Downloads/master.zip /home/lefv2603/Downloads/DeepLabv3FineTuning-master.zip
```
* copier to projet zippé sur le serveur beluga de Compute Canada
```
scp /home/lefv2603/Downloads/DeepLabv3FineTuning-master.zip vincelf@beluga.computecanada.ca:~
```
* Télécharger les modèles de resnet1010 et deeplabv3_resnet101_coco en avance depuis une autre machine que celle de Compute Canada car depuis le serveur de Compute Canada l'accès à ces URLs est bloqué. 
> Erreur si le modèle n'est pas dans le cache et doit être téléchargé à partir de Compute Canada
```
(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ python $TRAINDIR/main.py $TRAINDIR/CrackForest $TRAINDIR/VLFExp
Downloading: "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth" to /home/vincelf/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/urllib/request.py", line 1317, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 1244, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 1290, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 1239, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 1406, in connect
    super().connect()
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/socket.py", line 727, in create_connection
    raise err
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable
...
```
```
wget https://download.pytorch.org/models/resnet101-5d3b4d8f.pth
wget https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth
```
* copier les modèles sur le serveur de Compute Canada
```
scp /home/lefv2603/Downloads/resnet101-5d3b4d8f.pth vincelf@beluga.computecanada.ca:~
scp /home/lefv2603/Downloads/deeplabv3_resnet101_coco-586e9e4e.pth vincelf@beluga.computecanada.ca:~
```
* Déplacer les modèles dans le cache de torch
```
mv ~/resnet101-5d3b4d8f.pth /home/vincelf/.cache/torch/checkpoints
m ~/deeplabv3_resnet101_coco-586e9e4e.pth /home/vincelf/.cache/torch/checkpoints
```
* S'assurer d'avoir les bonnes librairies dans requirements.tx.
> __Note__: Les librairies peuvent être installées dans l'env virtuel Pyton à la main avec l'optoipn --no-index sinon il y a une erreur: 
```
(env) [vincelf@blg5602 DeepLabv3FineTuning-master-test]$ pip3 install torchvision
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx512, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting torchvision
  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b343e329450>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/torchvision/
```
```
tqdm
scikit-learn
opencv-python
torch
torchvision
```
Utiliser `pip3 install --no-index torchvision`

* Exécuter l'entrainement dans un mode intéractif (salloc)
```
salloc --account=def-germ2201-ab --gres=gpu:1 --cpus-per-task=10 --mem=48000M --time=0-3:00

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

#module load python/3.6 cuda cudnn
module load python/3.7.4 cuda cudnn

SOURCEDIR=~/gae724/src/DeepLabv3FineTuning-master-test
TRAINDIR=~/DeepLabv3FineTuning-master

# Prepare virtualenv
#virtualenv --no-download $SLURM_TMPDIR/env
python3 -m venv $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate
pip3 install --no-index -r $SOURCEDIR/requirements.txt


# Références : 
## https://expoundai.wordpress.com/2019/08/30/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch/
## https://github.com/msminhas93/DeepLabv3FineTuning
python $TRAINDIR/main.py $TRAINDIR/CrackForest $TRAINDIR/VLFExp
```

* Le modèle s'entraine
```
(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ python $TRAINDIR/main.py $TRAINDIR/CrackForest $TRAINDIR/VLFExp
Epoch 1/25
----------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.06it/s]
Train Loss: 0.0251
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.76it/s]
Test Loss: 0.0556
{'epoch': 1, 'Train_loss': 0.025081725791096687, 'Test_loss': 0.05564142018556595, 'Train_f1_score': 0.050840583267973714, 'Train_auroc': 0.5767874359848608, 'Test_f1_score': 0.010331824851978231, 'Test_auroc': 0.5188828199161867}
Epoch 2/25
----------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.07it/s]
Train Loss: 0.0155
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.91it/s]
Test Loss: 0.0385
{'epoch': 2, 'Train_loss': 0.015544182620942593, 'Test_loss': 0.03848109021782875, 'Train_f1_score': 0.1191172723341669, 'Train_auroc': 0.7760033667334525, 'Test_f1_score': 0.08732444335126234, 'Test_auroc': 0.7224661313356657}
...
Epoch 24/25
----------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.08it/s]
Train Loss: 0.0106
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.91it/s]
Test Loss: 0.0083
{'epoch': 24, 'Train_loss': 0.010582794435322285, 'Test_loss': 0.00830918364226818, 'Train_f1_score': 0.4236991146887413, 'Train_auroc': 0.9393122372904904, 'Test_f1_score': 0.3157004818295278, 'Test_auroc': 0.8214732776085168}
Epoch 25/25
----------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.08it/s]
Train Loss: 0.0087
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.90it/s]
Test Loss: 0.0072
{'epoch': 25, 'Train_loss': 0.008725304156541824, 'Test_loss': 0.007179579231888056, 'Train_f1_score': 0.3882697467070124, 'Train_auroc': 0.9296205158172175, 'Test_f1_score': 0.263064162183016, 'Test_auroc': 0.802269088719596}
Training complete in 10m 57s
Lowest Loss: 0.005794
```

* Sauver le modèle
```
# To save the model
torch.save(model, os.path.join(bpath, ‘weights.pt’))
```
* Charger le modèle
```
# Load the trained model
model = torch.load(‘./CFExp/weights.pt’)
# Set the model to evaluate mode
model.eval()
```

* Pour l'inférence il faut faire .eval

* How can this be extended to a single image having multiple masks (4 masks per image, where each mask represents a class) ?
Just use those masks as input and use crossentropy as the loss function

## Test de segmentation d'une image avec resnet101
Ce scrit fonctionne bien sur le serveur de Compute canada

* Se logguer sur le serveur beluga
* Préparer un répertoire pour les scripts
* démarrer une session intéractive
```salloc --account=def-germ2201-ab --gres=gpu:1 --cpus-per-task=10 --mem=48000M --time=0-3:00```
* charger les modules python 6.7, cuda et cudnn. À noter que sur le jetson nano (L4T Linux For Tegra), cuda et cudnn existent déjà, ils sont installés avec le Jetpack. Par contre je ne sais pas comment les "charger" pour que les librairies python y fasse référence, au lieu de retourner une erreur. 

```
#SBATCH --gres=gpu:1       # Request GPU "generic resources"; Number of GPU(s) per node
#SBATCH --cpus-per-task=10  # Cores proportional to GPUs: 6 on Cedar, 16 on Graham, 10 on Beluga.; CPU cores/threads
#SBATCH --mem=48000M       # Memory proportional to GPUs: 32000 Cedar, 64000 Graham, 48000M on Beluga.; memory per node; Requesting --mem=0 is interpreted by Slurm to mean "reserve all the available memory on each node assigned to the job."
#SBATCH --time=0-03:00
#SBATCH --output=%N-%j.out

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

#module load python/3.6 cuda cudnn
module load python/3.7.4 cuda cudnn

SOURCEDIR=~/gae724/src/DeepLabv3FineTuning-master-test
TRAINDIR=~/DeepLabv3FineTuning-master
```

* créer l'environnement virtuel python 3
```
#virtualenv --no-download $SLURM_TMPDIR/env
python3 -m venv $SLURM_TMPDIR/env
source $SLURM_TMPDIR/env/bin/activate
```
* installer les librairies python requisent
requirements.txt:
```
tqdm
scikit-learn
opencv-python
torch
torchvision
matplotlib
```
```
pip3 install --no-index -r $SOURCEDIR/requirements.txt
```

* exécuter le script
```
(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ python3 test_segmentation_resnet101.py
```
Il va créer une image plot.png

```
(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ cat test_segmentation_resnet101.py 
from torchvision import models
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as T
import numpy as np


def segment(net, path):
  img = Image.open(path)
  plt.imshow(img) 
  plt.axis('off') 
  plt.show()
  # Comment the Resize and CenterCrop for better inference results
  trf = T.Compose([T.Resize(256), 
                   T.CenterCrop(224), 
                   T.ToTensor(), 
                   T.Normalize(mean = [0.485, 0.456, 0.406], 
                               std = [0.229, 0.224, 0.225])])
  inp = trf(img).unsqueeze(0)
  out = net(inp)['out']
  om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()
  rgb = decode_segmap(om)
  plt.imshow(rgb) 
  plt.axis('off')
  plt.show()
  plt.savefig('plot.png')

# Define the helper function
def decode_segmap(image, nc=21):
  
  label_colors = np.array([(0, 0, 0),  # 0=background
               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle
               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),
               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow
               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),
               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person
               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),
               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor
               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])

  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  
  for l in range(0, nc):
    idx = image == l
    r[idx] = label_colors[l, 0]
    g[idx] = label_colors[l, 1]
    b[idx] = label_colors[l, 2]
    
  rgb = np.stack([r, g, b], axis=2)
  return rgb

fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()
segment(fcn, './test.jpeg')
```

## export to ONNX 
L'export vers un modele onnx fonctionne avec le script suivant. 

Les tests avec onnx et onnxruntime ne fonctionnent pas car les librairies n'existe pas dans le repo de Compute Canad (email en cours). 

Le model qui est utilisé est celui de DeepLabv3FineTuning-master-test qui a été sauvegardé avec 
torch.save(model, os.path.join(bpath, 'weights.pt')) (voir fichier /home/vincelf/DeepLabv3FineTuning-master/mains.py)

```
(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ cat export-to-onnx.py 
# Some standard imports
import io
import numpy as np

from torch import nn
import torch.utils.model_zoo as model_zoo
import torch.onnx

import torch.nn as nn
import torch.nn.init as init
 
#from model import createDeepLabv3

# Load pretrained model weights
#model_url = 'fcn-weights.pt'
batch_size = 1    # just a random number

# Initialize model with the pretrained weights
#torch_model = createDeepLabv3()
#torch_model = models.segmentation.deeplabv3_resnet101()
torch_model = torch.load('weights.pt')
#torch_model = torch.load_state_dict('weights')

# Set the model to evaluate mode
torch_model.eval()

#torch_model.cuda()
torch_model.to('cuda')

#Input to the model
# Reference:https://discuss.pytorch.org/t/random-number-on-gpu/9649 
# torch.cuda.FloatTensor(batch_size, 3, 224, 224).normal_() 
x = torch.randn(batch_size, 3, 224, 224, requires_grad=True).cuda()
#x.cuda()
#x.to('cuda')
torch_out = torch_model(x)

# Export the model
torch.onnx.export(torch_model,               # model being run
                  x,                         # model input (or a tuple for multiple inputs)
                  "test-onnx.onnx",   # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = ['output'], # the model's output names
                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes
                                'output' : {0 : 'batch_size'}})

import onnx

onnx_model = onnx.load("test-onnx.onnx")
onnx.checker.check_model(onnx_model)


import onnxruntime

ort_session = onnxruntime.InferenceSession("test-onnx.onnx")

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# compute ONNX Runtime output prediction
ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}
ort_outs = ort_session.run(None, ort_inputs)

# compare ONNX Runtime and PyTorch results
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

print("Exported model has been tested with ONNXRuntime, and the result looks good!")

(env) [vincelf@blg4101 DeepLabv3FineTuning-master-test]$ 
```

